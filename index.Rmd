---
title: "Estadística"
subtitle: "Aplicaciones de los momentos: entropía diferencial 2"
author: "Julio César Ramírez Pacheco"
date: "13/11/2023"
output:
  rmdformats::material:
    highlight: kate
    cards: false
---


```{r knitr_init, echo=FALSE, message=FALSE, warning=FALSE}
library(highcharter)
```



# Entropía diferencial

Sea $f(x)$ la densidad de probabilidad de un experimento aleatorio $\mathbb{E}$. Recordemos que la entropía de la función $f(x)$ (llamada entropía diferencial) está dada por la siguiente expresión:
$$
h(X) = -\int_{-\infty}^{+\infty}{f(x)\log(f(x))}.
$$

La entropía diferencial es pues, la entropía de Shannon para distribuciones que corresponden a variables aleatorias continuas, por ejemplo para la variable aleatoria uniforme, como se vió en la tarea pasada, la entropía tiene la siguiente relación densidad-entropía:
$$
h(f(x)=\frac{1}{b-a}) = \ln(b-a)
$$

y por lo tanto se puede notar que para el caso de la distribución uniforme al incrementar la varianza (cuando $a$ incrementa), se incrementa la entropía. La siguiente figura muestra lo anterior.

```{r eval=TRUE}
a          <- 0
b          <- seq(2,8, length=20)               # Variamos b
entropy    <- log(b-a) 
hc <- highchart() %>% 
  hc_add_series(cbind(b,entropy), name="UniformRV_entropy") %>%   hc_add_theme(hc_theme_smpl()) %>% 
  hc_title(text="Variacion de la entropia con la Varianza") %>%   hc_subtitle(text="Teoria de la informacion") %>%
  hc_xAxis(title=list(text="Valores de b")) %>%          hc_yAxis(title=list(text="Entropia de la funcion"))
hc

```

## Ejercicios

Investigar la relación varianza-entropia para las siguientes variables aleatorias continuas:

- Rayleigh
- Normal
- Exponencial
- Cauchy
- Laplace
- Logística
- Triangular

Para la variable aleatoria triangular, ?Existe una relación entre su moda y su entropía?


Rayleigh:

La entropía y la varianza no están directamente relacionadas. La entropía puede calcularse a partir de la distribución de probabilidad de Rayleigh, pero no hay una relación simple o directa entre la entropía y la varianza.

+Normal:

En la distribución normal, la entropía alcanza su máximo cuando la distribución es más plana, es decir, cuando su varianza es mayor. A medida que la distribución se acerca a ser más puntual (varianza más pequeña), la entropía disminuye.

+Exponencial:

En la distribución exponencial, la entropía está inversamente relacionada con el parámetro de escala. A medida que el parámetro de escala aumenta (lo que significa una varianza más pequeña), la entropía también disminuye.

+Cauchy:

La distribución de Cauchy es una distribución de colas pesadas y no tiene momentos definidos. Por lo tanto, la relación entre la varianza y la entropía puede ser más compleja o no trivial en esta distribución.

+Laplace:

La entropía en la distribución de Laplace también está inversamente relacionada con su parámetro de escala. A medida que la escala aumenta (lo que implica una varianza más pequeña), la entropía disminuye.

+Logística:

Similar a la distribución de Laplace, la entropía en la distribución logística está inversamente relacionada con su parámetro de escala.

+Triangular:

La distribución triangular tiene una relación más compleja entre su moda, la varianza y la entropía. En general, la entropía está relacionada con la forma y la dispersión de la distribución. No existe una relación directa entre la moda y la entropía, pero la forma específica de la distribución triangular y su dispersión afectarán la entropía.
En cuanto a la pregunta específica sobre la variable aleatoria triangular, no existe una relación directa entre su moda y su entropía. La entropía de una distribución triangular está influenciada por la forma y la dispersión de la distribución en su conjunto, no solo por su moda.

Nota: Para responder adecuadamente los anteriores cuestionamientos es necesario investigar las entropías de las variables aleatorias así como los valores de sus varianzas. De igual forma es necesario conocer el funcionamiento del paquete de `R` llamado `highcharter`.


# Entropía de Shannon discreta

La entropía mide el grado de complejidad de una variable aleatoria descrita por medio de su PDF o bién mediante su PMF. Para el caso discreto, la ecuación entrópica de Shannon está dada por:
$$
H(p) = -\sum_{k}{p_k \log(p_k)}
$$

Para la variable aleatoria Binomial, la PMF está dada por:
$$
\mbox{Pr}\{X=k\} = {n\choose k} p^k(1-p)^{n-k}
$$
y por lo tanto, la relación entre la entropía y la probabilidad $p$ está dada empíricamente como:

```{r eval=TRUE}
n          <- 20
x          <- 0:20
p          <- seq(0,1, length=20)
entropies  <- numeric(20)
for(i in 1:length(p))
{
  densities     <- dbinom(x,n,p[i])
  entropies[i]  <- -1*sum(densities*log(densities))
  
}
theoretical <- 0.5*log(2*pi*n*exp(1)*p*(1-p))
hc <- highchart() %>% 
  hc_add_series(cbind(p,entropies), name="BinomialRV_empirical") %>%  hc_add_series(cbind(p,theoretical), name="BinomialRV_theoretical") %>%  hc_add_theme(hc_theme_smpl()) %>% 
  hc_title(text="Variacion de la entropia contra p") %>%   hc_subtitle(text="Teoria de la informacion") %>%
  hc_xAxis(title=list(text="Valores de probabilidad p")) %>%          hc_yAxis(title=list(text="Entropia de la funcion"))
hc

```

## Ejercicios

Replicar el mismo procedimiento anterior para las siguientes variables aleatorias discretas:

- Binomial negativa.
- Geométrica.
- Poisson.
- Hipergeométrica.

```{r}
# Parámetros iniciales
n <- 20
x <- 0:20
p <- seq(0, 1, length = 20)

# Inicialización de vectores para almacenar entropías
entropies_negbin <- numeric(20)
entropies_geom <- numeric(20)
entropies_poisson <- numeric(20)
entropies_hypergeom <- numeric(20)

# Cálculo de entropías para la Binomial Negativa
for (i in 1:length(p)) {
  densities <- dnbinom(x, size = 5, prob = p[i])
  entropies_negbin[i] <- -1 * sum(densities * log(densities))
}

# Cálculo de entropías para la Geométrica
for (i in 1:length(p)) {
  densities <- dgeom(x, prob = p[i])
  entropies_geom[i] <- -1 * sum(densities * log(densities))
}

# Cálculo de entropías para la Poisson
for (i in 1:length(p)) {
  densities <- dpois(x, lambda = 5 * p[i])
  entropies_poisson[i] <- -1 * sum(densities * log(densities))
}

# Cálculo de entropías para la Hipergeométrica
for (i in 1:length(p)) {
  densities <- dhyper(x, m = 10, n = 10, k = 5 * p[i])
  entropies_hypergeom[i] <- -1 * sum(densities * log(densities))
}

# Cálculo teórico de entropías
theoretical_negbin <- 0.5 * log(2 * pi * 5 * exp(1) * p * (1 - p))
theoretical_geom <- 0.5 * log(2 * pi * (1 - p) / p^2)
theoretical_poisson <- 0.5 * log(2 * pi * 5 * exp(1) * p)
theoretical_hypergeom <- 0.5 * log(2 * pi * (10 - 5 * p) * (5 * p) / (10 * 10))

# Gráfico interactivo
hc <- highchart() %>% 
  hc_add_series(cbind(p, entropies_negbin), name = "NegBinRV_empirical") %>%  
  hc_add_series(cbind(p, theoretical_negbin), name = "NegBinRV_theoretical") %>%  
  hc_add_series(cbind(p, entropies_geom), name = "GeomRV_empirical") %>%  
  hc_add_series(cbind(p, theoretical_geom), name = "GeomRV_theoretical") %>%  
  hc_add_series(cbind(p, entropies_poisson), name = "PoissonRV_empirical") %>%  
  hc_add_series(cbind(p, theoretical_poisson), name = "PoissonRV_theoretical") %>%  
  hc_add_series(cbind(p, entropies_hypergeom), name = "HypergeomRV_empirical") %>%  
  hc_add_series(cbind(p, theoretical_hypergeom), name = "HypergeomRV_theoretical") %>%  
  hc_add_theme(hc_theme_smpl()) %>% 
  hc_title(text = "Variación de la entropía contra p") %>%   
  hc_subtitle(text = "Teoría de la información") %>%
  hc_xAxis(title = list(text = "Valores de probabilidad p")) %>%          
  hc_yAxis(title = list(text = "Entropía de la función"))

hc

```

